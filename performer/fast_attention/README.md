# Performer's Fast Attention (FAVOR+) Module.

See ["Rethinking Attention with Performers"](https://arxiv.org/abs/2009.14794) for the paper associated with this library, as well as the corresponding [Google AI Blog post](https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html).

We currently have FAVOR+ (Softmax and Generalized variants) written in Jax and Tensorflow.

If you found this codebase useful, please consider citing the paper:

```
@article{performer,
  author    = {Krzysztof Choromanski and
               Valerii Likhosherstov and
               David Dohan and
               Xingyou Song and
               Andreea Gane and
               Tam{\'{a}}s Sarl{\'{o}}s and
               Peter Hawkins and
               Jared Davis and
               Afroz Mohiuddin and
               Lukasz Kaiser and
               David Belanger and
               Lucy Colwell and
               Adrian Weller},
  title     = {Rethinking Attention with Performers},
  journal   = {CoRR},
  volume    = {abs/2009.14794},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.14794},
  archivePrefix = {arXiv},
  eprint    = {2009.14794}
}
```

